import logging
import os
import json
import time
import re
from datetime import datetime, timezone, timedelta
from analyzer.ai_analyzer import AIAnalyzer
from database.sheets_manager import SheetsManager

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def main():
    logger.info("=" * 60)
    logger.info("æ˜ åƒæ¡ˆä»¶ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° v1.22 [è¥¿æš¦ãƒ»å’Œæš¦ å³æ ¼è£œæ­£ç‰ˆ]")
    logger.info("=" * 60)
    
    try:
        from scrapers.direct_scraper import search_all_prefectures_direct
        from scrapers.content_extractor import ContentExtractor
        
        analyzer = AIAnalyzer()
        sheets_manager = SheetsManager(os.environ["SPREADSHEET_ID"], json.loads(os.environ["GCP_SERVICE_ACCOUNT"]))
        extractor = ContentExtractor()
        jst = timezone(timedelta(hours=9))
        today = datetime.now(jst).date()

        logger.info("ã€ã‚¹ãƒ†ãƒƒãƒ—1ã€‘å…¨å›½è‡ªæ²»ä½“ã‚µã‚¤ãƒˆã‹ã‚‰æœ€æ–°ãƒªãƒ³ã‚¯ã‚’åé›†...")
        prefecture_results = search_all_prefectures_direct()
        all_tasks = [{"pref": p, **r} for p, rs in prefecture_results.items() for r in rs]
        
        final_projects = []
        seen_titles = set()
        
        logger.info("ã€ã‚¹ãƒ†ãƒƒãƒ—2ã€‘æ¥µé™é¸åˆ¥ï¼ˆå½ã®2026å¹´æ¡ˆä»¶ã‚’å¾¹åº•æ’é™¤ï¼‰...")
        for i, task in enumerate(all_tasks, 1):
            url = task['url']
            title_raw = task['title']

            # --- ğŸ›¡ï¸ é–€ç•ª1ï¼šã‚¿ã‚¤ãƒˆãƒ«ã®ç‰©ç†æ’é™¤ (äº‹å¾Œå ±å‘Šãƒ»æ¡ç”¨ãƒ»éå»å¹´åº¦) ---
            if re.search(r"æ±ºå®š|å…¬è¡¨|é¸å®š|è½æœ­|çµæœ|å¯©æŸ»|å ±å‘Š|å®Ÿç¸¾", title_raw): continue # çµ‚ã‚ã£ãŸã‚‚ã®ã¯è¦‹ãªã„
            if re.search(r"æ¡ç”¨|è·å“¡|è–¬å‰¤å¸«|è­¦å¯Ÿ|æ•™å“¡|çœ‹è­·|è©¦é¨“|ç›¸è«‡", title_raw): continue # äººã®å‹Ÿé›†ã¯è¦‹ãªã„
            if re.search(r"ä»¤å’Œ[4-7]|R[4-7]|202[2-5]", title_raw) and "ä»¤å’Œ8" not in title_raw: continue

            # æœ€ä½é™å¿…è¦ãªã€Œä»•äº‹ã€ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            if not re.search(r"å…¬å‹Ÿ|å§”è¨—|å…¥æœ­|å‹Ÿé›†|ææ¡ˆ|ãƒ—ãƒ­ãƒãƒ¼ã‚¶ãƒ«|ã‚³ãƒ³ãƒš|åˆ¶ä½œ|æ’®å½±|æ¥­å‹™", title_raw): continue

            if i % 20 == 0: logger.info(f"é€²æ—: {i}/{len(all_tasks)} ä»¶å®Œäº†")
            
            # å†…å®¹æŠ½å‡º
            content_data = extractor.extract(url)
            if not content_data: continue
            raw_text = content_data['content']

            # --- ğŸ›¡ï¸ é–€ç•ª2ï¼šæœ¬æ–‡ã®ç”Ÿãƒ‡ãƒ¼ã‚¿æ¤œé–² (AIãŒå˜˜ã‚’ã¤ãå‰ã«ãƒã‚§ãƒƒã‚¯) ---
            # æœ¬æ–‡ã«ä»¤å’Œ6å¹´(2024)ã‚„ä»¤å’Œ7å¹´(2025)ãŒæ›¸ã„ã¦ã‚ã‚Šã€ä»¤å’Œ8å¹´(2026)ãŒãªã„å ´åˆã¯å³é™¤å¤–
            if re.search(r"ä»¤å’Œ[67]|R[67]|202[45]", raw_text) and not re.search(r"ä»¤å’Œ8|R8|2026", raw_text):
                continue

            # AIè§£æ
            analysis = analyzer.analyze_single(title_raw, raw_text, url)
            if not analysis: continue
            
            # --- ğŸ›¡ï¸ é–€ç•ª3ï¼šAIå›ç­”ã®çŸ›ç›¾ãƒã‚§ãƒƒã‚¯ ---
            if analysis.get('label') not in ["A", "B"]: continue
            title = analysis.get('title', 'ç„¡é¡Œ')
            if title in seen_titles: continue

            evidence = analysis.get('evidence','')
            memo = analysis.get('memo','')
            full_ans = f"{title} {evidence} {memo}"

            # AIãŒã€Œä»¤å’Œ8å¹´åº¦ã§ã¯ãªã„ã€ã¨æ›¸ã„ã¦ã„ã‚‹ã€ã¾ãŸã¯éå»ã ã¨èªã‚ã¦ã„ã‚‹å ´åˆ
            if re.search(r"ã§ã¯ã‚ã‚Šã¾ã›ã‚“|ã§ã¯ãªã„|éå»|çµ‚äº†|ä»¤å’Œ[67]|202[45]", memo + evidence):
                if "ä»¤å’Œ8" not in memo and "2026" not in memo: continue
                if re.search(r"ä»¤å’Œ8å¹´åº¦?ã®æ¡ˆä»¶ã§ã¯ã‚ã‚Šã¾ã›ã‚“", memo): continue

            # æœ¬æ–‡ã¨å›ç­”ã‚’åˆã‚ã›ã¦ã€Œ2026/ä»¤å’Œ8ã€ã®æ–‡å­—ãŒ1å›ã‚‚å‡ºãªã„ãªã‚‰é™¤å¤–
            if "ä»¤å’Œ8" not in full_ans and "2026" not in full_ans: continue

            # æœŸé™åˆ‡ã‚Œãƒã‚§ãƒƒã‚¯ (å¤‰æ›ãƒŸã‚¹å¯¾ç­–ï¼šAIãŒ2026å¹´ã¨ç­”ãˆã¦ã‚‚ã€å…ƒãŒä»¤å’Œ6å¹´ãªã‚‰ã“ã“ã§è½ã¡ã‚‹)
            deadline_str = analysis.get('deadline_prop', 'ä¸æ˜')
            if deadline_str == "ä¸æ˜": deadline_str = analysis.get('deadline_apply', 'ä¸æ˜')
            if deadline_str != "ä¸æ˜":
                m = re.search(r'(\d{4})[-/å¹´](\d{1,2})[-/æœˆ](\d{1,2})', deadline_str)
                if m:
                    # AIã®è¨ˆç®—ãƒŸã‚¹ã‚’è£œæ­£ï¼šã‚‚ã—è¥¿æš¦ãŒ2026ãªã®ã«å…ƒãŒR6ãªã‚‰ä¿®æ­£ã•ã‚Œã‚‹
                    d_date = datetime(int(m.group(1)), int(m.group(2)), int(m.group(3))).date()
                    if d_date < today: continue

            # --- âœ¨ æœ€çµ‚åˆæ ¼ï¼šæœ¬ç‰©ã®ä»¤å’Œ8å¹´åº¦ï¼ˆ2026å¹´åº¦ï¼‰æ¡ˆä»¶ ---
            analysis.update({'prefecture': task['pref']})
            final_projects.append(analysis)
            seen_titles.add(title)
            logger.info(f"ğŸ¯ çœŸã®2026å¹´æ¡ˆä»¶ã‚’æ•æ‰: {title}")
            time.sleep(0.1)

        if final_projects:
            sheet_name = datetime.now(jst).strftime("æ˜ åƒæ¡ˆä»¶_%Yå¹´%mæœˆ_v16")
            sheets_manager.append_projects(sheets_manager.prepare_v12_sheet(sheet_name), final_projects)
            logger.info(f"âœ¨ å®Œäº†ï¼ å³é¸æ¡ˆä»¶ {len(final_projects)}ä»¶ã‚’è¿½åŠ ã—ã¾ã—ãŸ")
        else:
            logger.warning("âš ï¸ 2026å¹´åº¦ã®æ–°è¦æ¡ˆä»¶ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    main()
