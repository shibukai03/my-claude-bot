import logging
import os
import json
import time
import re
from datetime import datetime, timezone, timedelta
from analyzer.ai_analyzer import AIAnalyzer
from database.sheets_manager import SheetsManager

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def main():
    logger.info("=" * 60)
    logger.info("æ˜ åƒæ¡ˆä»¶ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° v1.15 [YouTubeé™¤å¤–ãƒ»æ¡ˆä»¶ç‰¹åŒ–ç‰ˆ]")
    logger.info("=" * 60)
    
    try:
        from scrapers.direct_scraper import search_all_prefectures_direct
        from scrapers.content_extractor import ContentExtractor
        
        analyzer = AIAnalyzer()
        sheets_manager = SheetsManager(os.environ["SPREADSHEET_ID"], json.loads(os.environ["GCP_SERVICE_ACCOUNT"]))
        extractor = ContentExtractor()
        jst = timezone(timedelta(hours=9))
        today = datetime.now(jst).date()

        logger.info("ã€ã‚¹ãƒ†ãƒƒãƒ—1ã€‘å…¨å›½è‡ªæ²»ä½“ã‚µã‚¤ãƒˆã‹ã‚‰æœ€æ–°ãƒªãƒ³ã‚¯ã‚’åé›†é–‹å§‹...")
        prefecture_results = search_all_prefectures_direct()
        all_tasks = [{"pref": p, **r} for p, rs in prefecture_results.items() for r in rs]
        
        final_projects = []
        seen_titles = set()
        
        logger.info("ã€ã‚¹ãƒ†ãƒƒãƒ—2ã€‘æ¡ˆä»¶ã®é¸åˆ¥ï¼ˆYouTube/SNS/å…¬é–‹æ¸ˆå‹•ç”»ã‚’æ’é™¤ï¼‰...")
        for i, task in enumerate(all_tasks, 1):
            url = task['url']
            title_raw = task['title']

            # --- ğŸ›¡ï¸ ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼1ï¼šURLãƒ‰ãƒ¡ã‚¤ãƒ³ãƒã‚§ãƒƒã‚¯ ---
            # YouTubeã‚„SNSã®ãƒªãƒ³ã‚¯ãã®ã‚‚ã®ã¯ã€Œæ¡ˆä»¶ã€ã§ã¯ãªã„ãŸã‚å³é™¤å¤–
            if re.search(r"youtube\.com|youtu\.be|facebook\.com|instagram\.com|x\.com|twitter\.com", url):
                continue

            # --- ğŸ›¡ï¸ ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼2ï¼šã‚¿ã‚¤ãƒˆãƒ«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯ ---
            # ã€Œé…ä¿¡ä¸­ã€ã€Œå…¬é–‹ä¸­ã€ã€Œãƒãƒ£ãƒ³ãƒãƒ«ã€ã€Œå‹•ç”»ã‚’è¦‹ã‚‹ã€ãªã©ã¯æ¡ˆä»¶ã§ã¯ãªã„ãŸã‚é™¤å¤–
            if re.search(r"é…ä¿¡ä¸­|å…¬é–‹ä¸­|ãƒãƒ£ãƒ³ãƒãƒ«|è¦–è´ç”¨|å‹•ç”»ãƒ©ã‚¤ãƒ–ãƒ©ãƒª|å‹•ç”»é›†|ã”è¦§ãã ã•ã„", title_raw):
                continue

            if i % 10 == 0: logger.info(f"é€²æ—: {i}/{len(all_tasks)} ä»¶ãƒã‚§ãƒƒã‚¯ä¸­")
            
            # å†…å®¹æŠ½å‡º
            content_data = extractor.extract(url)
            if not content_data: continue
            
            # AIè§£æ
            analysis = analyzer.analyze_single(title_raw, content_data['content'], url)
            if not analysis: continue
            
            # --- ğŸ›¡ï¸ ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼3ï¼šAIå›ç­”ãƒ™ãƒ¼ã‚¹ã®å³é‡æ¤œé–² ---
            label = analysis.get('label', 'C')
            if label not in ["A", "B"]: continue
            
            title = analysis.get('title', 'ç„¡é¡Œ')
            if title in seen_titles: continue

            evidence = analysis.get('evidence', '')
            memo = analysis.get('memo', '')
            full_check_text = f"{title} {evidence} {memo}"

            # ã™ã§ã«å®Œæˆã—ãŸå‹•ç”»ã®ç´¹ä»‹ãƒšãƒ¼ã‚¸ï¼ˆéå»ã®æˆæœç‰©ï¼‰ã‚’æ’é™¤
            if re.search(r"åˆ¶ä½œã—ã¾ã—ãŸ|å…¬é–‹ã—ã¦ã„ã¾ã™|æ”¾æ˜ ä¸­|æ›´æ–°ã—ã¾ã—ãŸ|ãƒ©ã‚¤ãƒ–ãƒ©ãƒª", full_check_text):
                if not re.search(r"å§”è¨—|å‹Ÿé›†|å…¥æœ­|ãƒ—ãƒ­ãƒãƒ¼ã‚¶ãƒ«|ã‚³ãƒ³ãƒš", full_check_text):
                    continue

            # å¹´åº¦ãƒã‚§ãƒƒã‚¯ (2026å¹´/ä»¤å’Œ8å¹´ä»¥é™ã‚’å„ªå…ˆ)
            is_future = re.search(r"ä»¤å’Œ([8-9]|[1-9]\d)|202[6-9]|20[3-9]\d", full_check_text)
            is_past = re.search(r"ä»¤å’Œ[4-7]|R[4-7]|202[2-5]", full_check_text)
            if is_past and not is_future: continue

            # æœŸé™åˆ‡ã‚Œæ’é™¤
            deadline_str = analysis.get('deadline_prop', 'ä¸æ˜')
            if deadline_str == "ä¸æ˜": deadline_str = analysis.get('deadline_apply', 'ä¸æ˜')
            if deadline_str != "ä¸æ˜":
                match = re.search(r'(\d{4})[-/å¹´](\d{1,2})[-/æœˆ](\d{1,2})', deadline_str)
                if match:
                    deadline_date = datetime(int(match.group(1)), int(match.group(2)), int(match.group(3))).date()
                    if deadline_date < today: continue

            # --- âœ¨ åˆæ ¼ï¼ˆæœ¬ç‰©ã®æ¡ˆä»¶ï¼‰ ---
            analysis.update({'prefecture': task['pref']})
            final_projects.append(analysis)
            seen_titles.add(title)
            logger.info(f"âœ¨ æ¡ˆä»¶ç¢ºå®š: {title}")
            time.sleep(0.1)

        # 3. æ›¸ãè¾¼ã¿
        if final_projects:
            sheet_name = datetime.now(jst).strftime("æ˜ åƒæ¡ˆä»¶_%Yå¹´%mæœˆ_v16")
            sheets_manager.append_projects(sheets_manager.prepare_v12_sheet(sheet_name), final_projects)
            logger.info(f"âœ¨ å®Œäº†ï¼ {len(final_projects)}ä»¶ã‚’ã‚·ãƒ¼ãƒˆã«è¿½åŠ ã—ã¾ã—ãŸ")
        else:
            logger.warning("âš ï¸ å‹Ÿé›†ä¸­ã®æ–°è¦æ¡ˆä»¶ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    main()
