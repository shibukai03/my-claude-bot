import logging
import sys
import os
import json
import time
import re
from datetime import datetime, timezone, timedelta
from analyzer.ai_analyzer import AIAnalyzer
from database.sheets_manager import SheetsManager

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def main():
    logger.info("=" * 60)
    logger.info("æ˜ åƒæ¡ˆä»¶ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° v1.5 [ãƒ‡ãƒãƒƒã‚°å¼·åŒ–ç‰ˆ]")
    logger.info("=" * 60)
    
    try:
        from scrapers.direct_scraper import search_all_prefectures_direct
        from scrapers.content_extractor import ContentExtractor
        
        analyzer = AIAnalyzer()
        sheets_manager = SheetsManager(os.environ["SPREADSHEET_ID"], json.loads(os.environ["GCP_SERVICE_ACCOUNT"]))
        extractor = ContentExtractor()
        jst = timezone(timedelta(hours=9))
        today = datetime.now(jst).date()
        today_str = today.strftime("%Y-%m-%d")
        
        # 1. ãƒªãƒ³ã‚¯åé›†
        logger.info("ã€ã‚¹ãƒ†ãƒƒãƒ—1ã€‘ãƒªãƒ³ã‚¯åé›†é–‹å§‹")
        prefecture_results = search_all_prefectures_direct()
        
        all_tasks = []
        for pref, results in prefecture_results.items():
            for res in results:
                res['pref'] = pref
                all_tasks.append(res)
        
        logger.info(f"âœ… {len(all_tasks)} ä»¶ã®ãƒªãƒ³ã‚¯ã‚’åé›†")
        
        # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰: æœ€åˆã®10ä»¶ã®ã¿å‡¦ç†
        TEST_MODE = True
        if TEST_MODE:
            all_tasks = all_tasks[:10]
            logger.info(f"ğŸ§ª ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰: {len(all_tasks)}ä»¶ã®ã¿å‡¦ç†")
        
        # 2. ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡º
        logger.info("ã€ã‚¹ãƒ†ãƒƒãƒ—2ã€‘ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºé–‹å§‹")
        batch_requests = []
        url_map = {}
        
        for i, task in enumerate(all_tasks, 1):
            logger.info(f"æŠ½å‡ºé€²æ—: {i}/{len(all_tasks)} - {task['title'][:50]}")
            content_data = extractor.extract(task['url'])
            
            if not content_data:
                logger.warning(f"âš ï¸ æŠ½å‡ºå¤±æ•—: {task['url']}")
                continue
            
            # æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®æœ€åˆã®éƒ¨åˆ†ã‚’ãƒ­ã‚°å‡ºåŠ›ï¼ˆæœ€åˆã®3ä»¶ã®ã¿ï¼‰
            if i <= 3:
                logger.info(f"--- ã‚µãƒ³ãƒ—ãƒ« {i} ---")
                logger.info(f"ã‚¿ã‚¤ãƒˆãƒ«: {task['title']}")
                logger.info(f"ãƒ†ã‚­ã‚¹ãƒˆå†’é ­: {content_data.get('content', '')[:300]}...")
                logger.info("-" * 60)
            
            custom_id = f"req_{i}"
            url_map[custom_id] = task
            req = analyzer.make_batch_request(custom_id, task['title'], content_data.get('content', ''))
            batch_requests.append(req)
        
        if not batch_requests:
            logger.warning("âŒ è§£æå¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        logger.info(f"âœ… {len(batch_requests)}ä»¶ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºå®Œäº†")
        
        # 3. Batch APIãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
        logger.info("ã€ã‚¹ãƒ†ãƒƒãƒ—3ã€‘Batch APIãƒªã‚¯ã‚¨ã‚¹ãƒˆä½œæˆ")
        batch_file = "/tmp/batch_requests.jsonl"
        
        with open(batch_file, 'w', encoding='utf-8') as f:
            for req in batch_requests:
                f.write(json.dumps(req, ensure_ascii=False) + '\n')
        
        logger.info(f"âœ… ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ: {batch_file}")
        
        # 4. Batché€ä¿¡
        logger.info("ã€ã‚¹ãƒ†ãƒƒãƒ—4ã€‘Anthropic Batch APIé€ä¿¡")
        
        batch = analyzer.client.beta.messages.batches.create(requests=batch_file)
        
        batch_id = batch.id
        logger.info(f"âœ… Batché€ä¿¡å®Œäº† (ID: {batch_id})")
        
        # 5. å®Œäº†å¾…æ©Ÿ
        logger.info("ã€ã‚¹ãƒ†ãƒƒãƒ—5ã€‘å‡¦ç†å®Œäº†ã‚’å¾…æ©Ÿä¸­...")
        
        wait_count = 0
        while True:
            batch_status = analyzer.client.beta.messages.batches.retrieve(batch_id)
            status = batch_status.processing_status
            counts = batch_status.request_counts
            
            total = counts.succeeded + counts.errored + counts.canceled + counts.expired
            
            logger.info(f"â³ {status}: {total}/{len(batch_requests)}ä»¶å‡¦ç†æ¸ˆã¿ (æˆåŠŸ:{counts.succeeded}, ã‚¨ãƒ©ãƒ¼:{counts.errored})")
            
            if status == "ended":
                break
            
            wait_count += 1
            if wait_count > 60:
                logger.error("â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: 60åˆ†çµŒéã—ã¦ã‚‚å®Œäº†ã—ã¾ã›ã‚“ã§ã—ãŸ")
                return
            
            time.sleep(60)
        
        # 6. çµæœå–å¾—
        logger.info("ã€ã‚¹ãƒ†ãƒƒãƒ—6ã€‘çµæœå–å¾—ãƒ»è§£æ")
        
        # çµ±è¨ˆæƒ…å ±
        stats = {
            "label_a": 0,
            "label_b": 0,
            "label_c": 0,
            "errors": 0
        }
        
        label_c_reasons = []
        final_valid_projects = []
        
        # çµæœå–å¾—
        results_response = analyzer.client.beta.messages.batches.results(batch_id)
        
        for result in results_response:
            custom_id = result.custom_id
            
            if result.result.type == "succeeded":
                try:
                    message = result.result.message
                    res_text = message.content[0].text
                    
                    # JSONã‚’æŠ½å‡º
                    match = re.search(r'\{.*\}', res_text, re.DOTALL)
                    if not match:
                        logger.warning(f"âš ï¸ JSONæŠ½å‡ºå¤±æ•—: {custom_id}")
                        stats["errors"] += 1
                        continue
                    
                    analysis = json.loads(match.group(0))
                    label = analysis.get('label', 'C')
                    
                    # çµ±è¨ˆã‚’è¨˜éŒ²
                    if label == "A":
                        stats["label_a"] += 1
                    elif label == "B":
                        stats["label_b"] += 1
                    else:
                        stats["label_c"] += 1
                        # Label Cã®ç†ç”±ã‚’è¨˜éŒ²
                        orig_task = url_map.get(custom_id, {})
                        label_c_reasons.append({
                            "title": analysis.get('title', orig_task.get('title', 'ä¸æ˜'))[:100],
                            "evidence": analysis.get('evidence', 'ç†ç”±ä¸æ˜')[:200],
                            "memo": analysis.get('memo', '')[:100],
                            "deadline": analysis.get('deadline_prop', 'ä¸æ˜')
                        })
                    
                    # Label Aã¾ãŸã¯Bã®å ´åˆã¯ä¿å­˜å€™è£œ
                    if label in ["A", "B"]:
                        d_prop = analysis.get('deadline_prop', "ä¸æ˜")
                        
                        # ç· åˆ‡ãƒã‚§ãƒƒã‚¯
                        if d_prop and d_prop != "ä¸æ˜":
                            date_match = re.search(r'(\d{4})[-/å¹´](\d{1,2})[-/æœˆ](\d{1,2})', d_prop)
                            if date_match:
                                y, m, d = map(int, date_match.groups())
                                deadline_date = datetime(y, m, d).date()
                                
                                if deadline_date < today:
                                    logger.info(f"â© ç· åˆ‡è¶…éã®ãŸã‚é™¤å¤–: {analysis.get('title')} (ç· åˆ‡:{d_prop})")
                                    continue
                        
                        # åˆæ ¼
                        orig = url_map[custom_id]
                        analysis['source_url'] = orig['url']
                        analysis['prefecture'] = orig['pref']
                        final_valid_projects.append(analysis)
                        
                        logger.info(f"âœ… åˆæ ¼ [Label {label}]: {analysis.get('title')} (ç· åˆ‡:{d_prop})")
                
                except json.JSONDecodeError as e:
                    logger.error(f"âŒ JSONè§£æã‚¨ãƒ©ãƒ¼ ({custom_id}): {e}")
                    stats["errors"] += 1
                except Exception as e:
                    logger.error(f"âŒ å‡¦ç†ã‚¨ãƒ©ãƒ¼ ({custom_id}): {e}")
                    stats["errors"] += 1
            
            elif result.result.type == "errored":
                logger.error(f"âŒ API error: {custom_id}")
                stats["errors"] += 1
        
        # è©³ç´°ãªçµ±è¨ˆã¨ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’å‡ºåŠ›
        logger.info("=" * 60)
        logger.info("ğŸ“Š åˆ¤å®šçµæœã®çµ±è¨ˆ")
        logger.info("=" * 60)
        logger.info(f"Label A (æœ€å„ªå…ˆ): {stats['label_a']}ä»¶")
        logger.info(f"Label B (é€šå¸¸): {stats['label_b']}ä»¶")
        logger.info(f"Label C (é™¤å¤–): {stats['label_c']}ä»¶")
        logger.info(f"ã‚¨ãƒ©ãƒ¼: {stats['errors']}ä»¶")
        logger.info(f"åˆæ ¼æ¡ˆä»¶: {len(final_valid_projects)}ä»¶")
        logger.info("=" * 60)
        
        # Label Cã®ç†ç”±ã‚’å‡ºåŠ›ï¼ˆæœ€å¤§10ä»¶ï¼‰
        if label_c_reasons:
            logger.info("ğŸ” é™¤å¤–ã•ã‚ŒãŸæ¡ˆä»¶ã®ç†ç”±ï¼ˆã‚µãƒ³ãƒ—ãƒ«10ä»¶ï¼‰")
            logger.info("=" * 60)
            for i, reason in enumerate(label_c_reasons[:10], 1):
                logger.info(f"{i}. ã‚¿ã‚¤ãƒˆãƒ«: {reason['title']}")
                logger.info(f"   ç· åˆ‡: {reason['deadline']}")
                logger.info(f"   è¨¼æ‹ : {reason['evidence']}")
                logger.info(f"   ãƒ¡ãƒ¢: {reason['memo']}")
                logger.info("-" * 60)
        
        # 7. ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ›¸ãè¾¼ã¿
        if final_valid_projects:
            logger.info("ã€ã‚¹ãƒ†ãƒƒãƒ—7ã€‘ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ›¸ãè¾¼ã¿")
            sheet_name = datetime.now(jst).strftime("æ˜ åƒæ¡ˆä»¶_%Yå¹´%mæœˆ_v15")
            worksheet = sheets_manager.prepare_v12_sheet(sheet_name)
            sheets_manager.append_projects(worksheet, final_valid_projects)
            logger.info(f"âœ¨ å®Œäº†ï¼ {len(final_valid_projects)}ä»¶ã‚’ã‚·ãƒ¼ãƒˆã«è¿½åŠ ")
        else:
            logger.warning("âš ï¸ å¿œå‹Ÿå¯èƒ½ãªæ˜ åƒæ¡ˆä»¶ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            logger.info("ğŸ’¡ ä¸Šè¨˜ã®é™¤å¤–ç†ç”±ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        
    except Exception as e:
        logger.error(f"âŒ ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        sys.exit(1)

if __name__ == "__main__":
    main()
