import logging
import os
import json
import time
import re
from datetime import datetime, timezone, timedelta
from analyzer.ai_analyzer import AIAnalyzer
from database.sheets_manager import SheetsManager

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def main():
    logger.info("=" * 60)
    logger.info("æ˜ åƒæ¡ˆä»¶ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° v1.19 [ãƒ—ãƒ­æ¡ˆä»¶ç‰¹åŒ–ãƒ»ä¸ç´”ç‰©ã‚¼ãƒ­ç›®æ¨™]")
    logger.info("=" * 60)
    
    try:
        from scrapers.direct_scraper import search_all_prefectures_direct
        from scrapers.content_extractor import ContentExtractor
        
        analyzer = AIAnalyzer()
        sheets_manager = SheetsManager(os.environ["SPREADSHEET_ID"], json.loads(os.environ["GCP_SERVICE_ACCOUNT"]))
        extractor = ContentExtractor()
        jst = timezone(timedelta(hours=9))
        today = datetime.now(jst).date()

        logger.info("ã€ã‚¹ãƒ†ãƒƒãƒ—1ã€‘å…¨å›½è‡ªæ²»ä½“ã‚µã‚¤ãƒˆã‹ã‚‰æœ€æ–°ãƒªãƒ³ã‚¯ã‚’åé›†...")
        prefecture_results = search_all_prefectures_direct()
        all_tasks = [{"pref": p, **r} for p, rs in prefecture_results.items() for r in rs]
        
        final_projects = []
        seen_titles = set()
        
        logger.info("ã€ã‚¹ãƒ†ãƒƒãƒ—2ã€‘æ¡ˆä»¶ã®é¸åˆ¥ï¼ˆå…¬å‹Ÿãƒ»å§”è¨—ãƒ»2026å¹´åº¦äºˆç®—ã«å³é¸ï¼‰...")
        for i, task in enumerate(all_tasks, 1):
            url = task['url']
            title_raw = task['title']

            # --- ğŸ›¡ï¸ é–€ç•ª1ï¼šã€ç‰©ç†æ’é™¤ã€‘URLã¨ãƒ‰ãƒ¡ã‚¤ãƒ³ ---
            if re.search(r"youtube\.com|youtu\.be|facebook\.com|instagram\.com|x\.com|twitter\.com", url):
                continue

            # --- ğŸ›¡ï¸ é–€ç•ª2ï¼šã€ã‚¿ã‚¤ãƒˆãƒ«ã«ã‚ˆã‚‹çµ¶å¯¾é™¤å¤–ãƒ«ãƒ¼ãƒ«ã€‘ ---
            # 1. è·å“¡æ¡ç”¨ãƒ»è³‡æ ¼è©¦é¨“ï¼ˆæ˜ åƒåˆ¶ä½œä¼šç¤¾ãŒå—ã‘ã‚‹ä»•äº‹ã§ã¯ãªã„ã‚‚ã®ï¼‰
            if re.search(r"æ¡ç”¨|è·å“¡å‹Ÿé›†|è–¬å‰¤å¸«|è­¦å¯Ÿå®˜|æ•™å“¡å‹Ÿé›†|ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹|è©¦é¨“|ç›¸è«‡ä¼š", title_raw):
                continue
            # 2. äº‹å¾Œå ±å‘Šãƒ»çµæœï¼ˆã™ã§ã«å¿œå‹Ÿã§ããªã„ã‚‚ã®ï¼‰
            if re.search(r"æ±ºå®šã—ã¾ã—ãŸ|é¸å®šçµæœ|å¯©æŸ»çµæœ|å…¥æœ­çµæœ|è½æœ­|äº‹å¾Œå ±å‘Š|æ›´æ–°ã—ã¾ã—ãŸ|é…ä¿¡ä¸­|å…¬é–‹ä¸­", title_raw):
                continue
            # 3. å¹´åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ (ä»¤å’Œ7å¹´ä»¥å‰ãŒãƒ¡ã‚¤ãƒ³ã®ã‚¿ã‚¤ãƒˆãƒ«ã¯ç„¡è¦–)
            if re.search(r"ä»¤å’Œ[4-7]|R[4-7]|202[2-5]", title_raw) and "ä»¤å’Œ8" not in title_raw:
                continue

            # --- ğŸ›¡ï¸ é–€ç•ª3ï¼šã€æ¡ˆä»¶å½¢å¼ã®å¿…é ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€‘ ---
            # ã€Œå…¬å‹Ÿã€ã€Œå§”è¨—ã€ã€Œå…¥æœ­ã€ã€Œãƒ—ãƒ­ãƒãƒ¼ã‚¶ãƒ«ã€ã®ã„ãšã‚Œã‹ãŒãªã„ãƒšãƒ¼ã‚¸ã¯ã€å˜ãªã‚‹ãŠçŸ¥ã‚‰ã›ã¨ã—ã¦æ¨ã¦ã‚‹
            if not re.search(r"å…¬å‹Ÿ|å§”è¨—|å…¥æœ­|å‹Ÿé›†|ãƒ—ãƒ­ãƒãƒ¼ã‚¶ãƒ«|ã‚³ãƒ³ãƒš|ä¼ç”»ææ¡ˆ", title_raw):
                continue

            if i % 20 == 0: logger.info(f"é€²æ—: {i}/{len(all_tasks)} ä»¶å®Œäº†")
            
            # å†…å®¹æŠ½å‡º
            content_data = extractor.extract(url)
            if not content_data: continue
            
            # AIè§£æ
            analysis = analyzer.analyze_single(title_raw, content_data['content'], url)
            if not analysis: continue
            
            # --- ğŸ›¡ï¸ é–€ç•ª4ï¼šã€AIè§£æå¾Œã®å†…å®¹æ¤œé–²ã€‘ ---
            if analysis.get('label') not in ["A", "B"]: continue
            title = analysis.get('title', 'ç„¡é¡Œ')
            if title in seen_titles: continue

            evidence = analysis.get('evidence','')
            memo = analysis.get('memo','')
            full_text = f"{title} {evidence} {memo}"

            # â‘  å¦å®šèªãƒ»äº‹å¾Œå ±å‘Šã®å†æ¤œé–²
            if re.search(r"ã§ã¯ã‚ã‚Šã¾ã›ã‚“|ã§ã¯ãªã„|éå»ã®æ¡ˆä»¶|çµ‚äº†ã—ã¦ã„ã¾ã™|æ±ºå®šã—ã¾ã—ãŸ|å€™è£œè€…ã‚’é¸å®š", full_text):
                continue

            # â‘¡ ã€Œä»¤å’Œ8å¹´/2026å¹´ã€ã®ã€Œæ–°è¦æ¡ˆä»¶ã€ã§ã‚ã‚‹ã“ã¨ã®è¨¼æ˜
            # æœ¬æ–‡ã«ä»¤å’Œ8å¹´(2026å¹´)ãŒã€Œã“ã‚Œã‹ã‚‰å§‹ã¾ã‚‹ã€æ–‡è„ˆã§ãªã„ã‚‚ã®ã¯é™¤å¤–
            if "ä»¤å’Œ8" not in full_text and "2026" not in full_text:
                continue
            
            # éå»ã®å¹´åº¦(ä»¤å’Œ7)ãŒå¼·èª¿ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€æœªæ¥æ¡ˆä»¶ã§ã‚‚é™¤å¤–ï¼ˆæ··åŒã‚’é¿ã‘ã‚‹ï¼‰
            if re.search(r"ä»¤å’Œ7å¹´åº¦ã®æ¡ˆä»¶|ä»¤å’Œ7å¹´åº¦äºˆç®—", full_text) and "ä»¤å’Œ8" not in full_text:
                continue

            # â‘¢ æœŸé™åˆ‡ã‚Œãƒã‚§ãƒƒã‚¯
            deadline_str = analysis.get('deadline_prop', 'ä¸æ˜')
            if deadline_str == "ä¸æ˜": deadline_str = analysis.get('deadline_apply', 'ä¸æ˜')
            if deadline_str != "ä¸æ˜":
                match = re.search(r'(\d{4})[-/å¹´](\d{1,2})[-/æœˆ](\d{1,2})', deadline_str)
                if match:
                    d_date = datetime(int(match.group(1)), int(match.group(2)), int(match.group(3))).date()
                    if d_date < today: continue

            # --- âœ¨ åˆæ ¼ï¼ˆæ˜ åƒåˆ¶ä½œæ¥­ã¨ã—ã¦æˆç«‹ã™ã‚‹å…¬å‹Ÿæ¡ˆä»¶ï¼‰ ---
            analysis.update({'prefecture': task['pref']})
            final_projects.append(analysis)
            seen_titles.add(title)
            logger.info(f"ğŸ¯ çœŸã®æ¡ˆä»¶ã‚’æ•æ‰: {title}")
            time.sleep(0.1)

        # 3. æ›¸ãè¾¼ã¿
        if final_projects:
            sheet_name = datetime.now(jst).strftime("æ˜ åƒæ¡ˆä»¶_%Yå¹´%mæœˆ_v16")
            sheets_manager.append_projects(sheets_manager.prepare_v12_sheet(sheet_name), final_projects)
            logger.info(f"âœ¨ å®Œäº†ï¼ å³é¸ã•ã‚ŒãŸ {len(final_projects)}ä»¶ã‚’è¿½åŠ ã—ã¾ã—ãŸ")
        else:
            logger.warning("âš ï¸ ç¾åœ¨å‹Ÿé›†ä¸­ã®æœ‰åŠ¹ãªæ¡ˆä»¶ã¯è¦‹é€ã‚‰ã‚Œã¾ã—ãŸ")
            
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    main()
