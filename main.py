import logging
import os
import json
import time
import re
import unicodedata
from datetime import datetime, timezone, timedelta
from concurrent.futures import ThreadPoolExecutor # ğŸ†• ä¸¦åˆ—å‡¦ç†ã®å¸ä»¤å¡”
from analyzer.ai_analyzer import AIAnalyzer
from database.sheets_manager import SheetsManager

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- ğŸ†• ä½œæ¥­å“¡ï¼ˆ1ä»¶ã®æ¡ˆä»¶ã‚’å¾¹åº•çš„ã«èª¿ã¹ã‚‹é–¢æ•°ï¼‰ ---
def process_task(task, extractor, analyzer, today):
    url = task['url']
    title_raw = task['title']
    pref = task['pref']

    # ğŸ›¡ï¸ é–€ç•ª1ï¼šSNSé™¤å¤– ï¼† ã‚¿ã‚¤ãƒˆãƒ«ã®ã€Œå†·å¾¹ãªæ’é™¤ã€
    if re.search(r"youtube\.com|youtu\.be|facebook\.com|instagram\.com|x\.com|twitter\.com", url): return None
    if re.search(r"æ±ºå®š|å…¬è¡¨|é¸å®š|è½æœ­|çµæœ|å¯©æŸ»|å ±å‘Š|å®Ÿç¸¾|æˆåŠŸ|é”æˆ|å…¬é–‹|å®Œäº†|åˆ¶ä½œã—ã¾ã—ãŸ|æ”¾æ˜ ä¸­|é…ä¿¡ä¸­|çµ‚äº†", title_raw):
        return None
    if re.search(r"æ¡ç”¨|è·å“¡|è–¬å‰¤å¸«|è­¦å¯Ÿ|æ•™å“¡|çœ‹è­·|åŒ»å¸«|è©¦é¨“|ç›¸è«‡|å€‹äºº|è¬›ç¿’", title_raw):
        return None

    # ğŸ›¡ï¸ é–€ç•ª2ï¼šã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–æ¡ˆä»¶ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã“ã‚ŒãŒãªã„ã¨AIã«é€ã‚‰ãªã„ï¼‰
    if not re.search(r"å‹Ÿé›†|å§”è¨—|å…¥æœ­|ãƒ—ãƒ­ãƒãƒ¼ã‚¶ãƒ«|ã‚³ãƒ³ãƒš|å…¬å‹Ÿ|ä¼ç”»ææ¡ˆ|åˆ¶ä½œ|ä½œæˆ|æ’®å½±|æ¥­å‹™|å‹•ç”»|PR|ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³", title_raw):
        return None

    # ãƒšãƒ¼ã‚¸å†…å®¹ã®å–å¾—
    content_data = extractor.extract(url)
    if not content_data: return None
    
    # å…¨è§’åŠè§’ã®æ­£è¦åŒ–
    normalized_text = unicodedata.normalize('NFKC', content_data['content'])

    # ğŸ›¡ï¸ é–€ç•ª3ï¼šå¹´åº¦æ¤œé–²ï¼ˆä»¤å’Œ8å¹´åº¦ã‚’æ•‘æ¸ˆï¼‰
    if not re.search(r"ä»¤å’Œ[789]|R[789]|202[567]", normalized_text):
        return None

    # AIè§£æ (Haiku 4.5)
    analysis = analyzer.analyze_single(title_raw, normalized_text, url)
    if not analysis: return None
    
    # ğŸ›¡ï¸ é–€ç•ª4ï¼šAIå›ç­”ã®æœ€çµ‚ç²¾æŸ»
    if analysis.get('label') not in ["A", "B"]: return None
    
    # â‘  æœŸé™åˆ‡ã‚Œãƒã‚§ãƒƒã‚¯ (ã‚¾ãƒ³ãƒ“æ¡ˆä»¶ã‚’æ•°å­¦çš„ã«é™¤å¤–)
    dates_to_check = []
    for key in ['deadline_apply', 'deadline_prop']:
        d_str = analysis.get(key, 'ä¸æ˜')
        if d_str and d_str != "ä¸æ˜":
            m = re.search(r'(\d{4})[-/å¹´](\d{1,2})[-/æœˆ](\d{1,2})', d_str)
            if m: dates_to_check.append(datetime(int(m.group(1)), int(m.group(2)), int(m.group(3))).date())
    
    if dates_to_check and all(d < today for d in dates_to_check):
        # logger.info(f"âŒ› æœŸé™åˆ‡ã‚Œé™¤å¤–: {title_raw}") # ä¸¦åˆ—æ™‚ã¯ãƒ­ã‚°ãŒæ··ã–ã‚‹ã®ã§æŠ‘åˆ¶
        return None

    # â‘¡ ä»¤å’Œ8å¹´åº¦(2026)ã®æ¡ˆä»¶ã§ã‚ã‚‹ã“ã¨ã‚’æœ€çµ‚ç¢ºèª
    evidence = analysis.get('evidence','')
    memo = analysis.get('memo','')
    full_ans = f"{title_raw} {evidence} {memo}"
    if re.search(r"ä»¤å’Œ7å¹´åº¦?ã®æ¡ˆä»¶|ä»¤å’Œ7å¹´åº¦äºˆç®—ã®ã¿", memo) and "ä»¤å’Œ8" not in full_ans:
        return None

    # ã™ã¹ã¦ã®é–¢é–€ã‚’çªç ´ï¼
    analysis.update({'prefecture': pref})
    return analysis

# --- ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ã‚¸ãƒ³ ---
def main():
    logger.info("=" * 60)
    logger.info("æ˜ åƒæ¡ˆä»¶ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° v1.29 [ä¸¦åˆ—é«˜é€Ÿãƒ»å…¨é–€ç•ªç¶™æ‰¿ç‰ˆ]")
    logger.info("=" * 60)
    
    try:
        from scrapers.direct_scraper import search_all_prefectures_direct
        from scrapers.content_extractor import ContentExtractor
        
        analyzer = AIAnalyzer()
        sheets_manager = SheetsManager(os.environ["SPREADSHEET_ID"], json.loads(os.environ["GCP_SERVICE_ACCOUNT"]))
        extractor = ContentExtractor()
        jst = timezone(timedelta(hours=9))
        today = datetime.now(jst).date()

        logger.info("ã€ã‚¹ãƒ†ãƒƒãƒ—1ã€‘å…¨å›½è‡ªæ²»ä½“ã‚µã‚¤ãƒˆã‹ã‚‰æœ€æ–°ãƒªãƒ³ã‚¯ã‚’åé›†...")
        prefecture_results = search_all_prefectures_direct()
        all_tasks = [{"pref": p, **r} for p, rs in prefecture_results.items() for r in rs]
        
        logger.info(f"ã€ã‚¹ãƒ†ãƒƒãƒ—2ã€‘æ¡ˆä»¶é¸åˆ¥ï¼ˆ10ä»¶ä¸¦åˆ—å®Ÿè¡Œä¸­... / å…¨ {len(all_tasks)}ä»¶ï¼‰")
        
        final_projects = []
        seen_titles = set()

        # ğŸ†• ä¸¦åˆ—å®Ÿè¡Œã®é­”æ³•ï¼š10äººã®ä½œæ¥­å“¡ãŒåŒæ™‚ã« process_task ã‚’å®Ÿè¡Œã—ã¾ã™
        with ThreadPoolExecutor(max_workers=10) as executor:
            # mapã‚„submitã‚’ä½¿ã£ã¦ä¸€æ°—ã«ä»•äº‹ã‚’æŠ•ã’ã‚‹
            futures = [executor.submit(process_task, task, extractor, analyzer, today) for task in all_tasks]
            
            for future in futures:
                result = future.result()
                if result:
                    title = result.get('title', 'ç„¡é¡Œ')
                    if title not in seen_titles:
                        final_projects.append(result)
                        seen_titles.add(title)
                        logger.info(f"ğŸ¯ çœŸã®æ¡ˆä»¶ã‚’æ•æ‰: {title}")

        if final_projects:
            sheet_name = datetime.now(jst).strftime("æ˜ åƒæ¡ˆä»¶_%Yå¹´%mæœˆ_v16")
            sheets_manager.append_projects(sheets_manager.prepare_v12_sheet(sheet_name), final_projects)
            logger.info(f"âœ¨ å®Œäº†ï¼ çœŸã®æœ‰åŠ¹æ¡ˆä»¶ {len(final_projects)}ä»¶ã‚’è¿½åŠ ã—ã¾ã—ãŸ")
        else:
            logger.warning("âš ï¸ ç¾åœ¨å‹Ÿé›†ä¸­ã®æœ‰åŠ¹æ¡ˆä»¶ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    main()
