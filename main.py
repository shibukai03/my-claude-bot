"""
è¡Œæ”¿æ˜ åƒæ¡ˆä»¶ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 
ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆï¼ˆç›´æ¥ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ç‰ˆï¼‰
"""

import logging
import sys
import os

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    logger.info("=" * 60)
    logger.info("æ˜ åƒæ¡ˆä»¶ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹ï¼ˆç›´æ¥ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ–¹å¼ï¼‰")
    logger.info("=" * 60)
    
    try:
        # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
        from scrapers.direct_scraper import search_all_prefectures_direct
        from scrapers.content_extractor import ContentExtractor
        from analyzer.ai_analyzer import AIAnalyzer
        from database.sheets_manager import SheetsManager
        
        logger.info("ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
        
        # åˆæœŸåŒ–
        extractor = ContentExtractor()
        analyzer = AIAnalyzer()
        sheets_manager = SheetsManager()
        
        logger.info("åˆæœŸåŒ–å®Œäº†")
        
        # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚’é–‹ã
        sheet_name = sheets_manager.create_monthly_sheet()
        sheets_manager.open_sheet(sheet_name)
        
        logger.info(f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæº–å‚™å®Œäº†: {sheet_name}")
        
        # ã‚¹ãƒ†ãƒƒãƒ—1: ç›´æ¥ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        logger.info("ã€ã‚¹ãƒ†ãƒƒãƒ—1ã€‘éƒ½é“åºœçœŒã‚µã‚¤ãƒˆã‹ã‚‰ç›´æ¥ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°")
        prefecture_results = search_all_prefectures_direct()
        
        # URLãƒªã‚¹ãƒˆã‚’å¹³å¦åŒ–
        all_urls = []
        for pref_name, results in prefecture_results.items():
            for result in results:
                result['prefecture'] = pref_name
                all_urls.append(result)
        
        logger.info(f"åˆè¨ˆ {len(all_urls)} ä»¶ã®URLã‚’ç™ºè¦‹")
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡º
        logger.info("ã€ã‚¹ãƒ†ãƒƒãƒ—2ã€‘ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºé–‹å§‹")

all_contents = []
processed_urls = set()

max_process = min(100, len(all_urls))
for idx, url_data in enumerate(all_urls[:max_process], 1):
    logger.info(f"æŠ½å‡ºé€²æ—: {idx}/{max_process}")
    
    url = url_data['url']
    
    # æ—¢ã«å‡¦ç†æ¸ˆã¿ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
    if url in processed_urls:
        continue
    
    processed_urls.add(url)
    
    # HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡º
    extracted = extractor.extract(url)
    
    if extracted:
        extracted['prefecture'] = url_data['prefecture']
        all_contents.append(extracted)
        logger.info(f"âœ“ HTMLæŠ½å‡º: {extracted['title'][:50]}")
        
        # PDFãƒªãƒ³ã‚¯ã‚‚å‡¦ç†
        pdf_links = extracted.get('pdf_links', [])
        if pdf_links:
            logger.info(f"  ğŸ“„ PDFç™ºè¦‹: {len(pdf_links)}ä»¶")
            
            for pdf_idx, pdf_url in enumerate(pdf_links[:3], 1):  # æœ€å¤§3ä»¶
                if pdf_url in processed_urls:
                    continue
                
                processed_urls.add(pdf_url)
                
                logger.info(f"  ğŸ“„ PDFæŠ½å‡ºä¸­ ({pdf_idx}/3): {pdf_url.split('/')[-1][:30]}")
                pdf_content = extractor.extract(pdf_url)
                
                if pdf_content:
                    pdf_content['prefecture'] = url_data['prefecture']
                    all_contents.append(pdf_content)
                    logger.info(f"  âœ“ PDFæŠ½å‡ºæˆåŠŸ")

logger.info(f"ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºå®Œäº†: {len(all_contents)}ä»¶ï¼ˆHTML + PDFï¼‰")
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: AIè§£æ
        if all_contents:
            logger.info("ã€ã‚¹ãƒ†ãƒƒãƒ—3ã€‘AIè§£æé–‹å§‹")
            analyzed_results = analyzer.batch_analyze(all_contents)
            logger.info(f"æ˜ åƒæ¡ˆä»¶æŠ½å‡º: {len(analyzed_results)}ä»¶")
        else:
            logger.warning("æŠ½å‡ºã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒã‚ã‚Šã¾ã›ã‚“")
            analyzed_results = []
        
        # ã‚¹ãƒ†ãƒƒãƒ—4: ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ä¿å­˜
        if analyzed_results:
            logger.info("ã€ã‚¹ãƒ†ãƒƒãƒ—4ã€‘ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆä¿å­˜é–‹å§‹")
            added = sheets_manager.append_projects(analyzed_results)
            logger.info(f"âœ“ ä¿å­˜å®Œäº†: {added}ä»¶è¿½åŠ ")
        else:
            logger.warning("ä¿å­˜ã™ã‚‹æ˜ åƒæ¡ˆä»¶ãŒã‚ã‚Šã¾ã›ã‚“")
        
        # ã‚µãƒãƒªãƒ¼
        logger.info("=" * 60)
        logger.info("å®Ÿè¡Œå®Œäº†")
        logger.info(f"ç™ºè¦‹URLæ•°: {len(all_urls)}ä»¶")
        logger.info(f"ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡º: {len(all_contents)}ä»¶")
        logger.info(f"æ˜ åƒæ¡ˆä»¶: {len(analyzed_results)}ä»¶")
        logger.info("=" * 60)
        
    except Exception as e:
        logger.error(f"ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()
