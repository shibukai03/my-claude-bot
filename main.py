import logging
import sys
import os
import json
import time
import re
from datetime import datetime, timezone, timedelta
from analyzer.ai_analyzer import AIAnalyzer
from database.sheets_manager import SheetsManager

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def main():
    start_time = time.time() # ğŸ†• é–‹å§‹æ™‚åˆ»ã‚’è¨˜éŒ²
    logger.info("=" * 60)
    logger.info("æ˜ åƒæ¡ˆä»¶ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° v1.10 [Batch/é€šå¸¸APIãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç‰ˆ]")
    logger.info("=" * 60)
    
    try:
        from scrapers.direct_scraper import search_all_prefectures_direct
        from scrapers.content_extractor import ContentExtractor
        
        analyzer = AIAnalyzer()
        sheets_manager = SheetsManager(os.environ["SPREADSHEET_ID"], json.loads(os.environ["GCP_SERVICE_ACCOUNT"]))
        extractor = ContentExtractor()
        jst = timezone(timedelta(hours=9))
        today = datetime.now(jst).date()

        # ã€æ•‘æ¸ˆã€‘æœªå®Œäº†ãƒãƒƒãƒãŒãªã„ã‹ç¢ºèª
        batch_id = None
        url_map = {} 
        existing_batches = analyzer.client.beta.messages.batches.list(limit=5)
        for b in existing_batches.data:
            if b.processing_status != "ended" or (datetime.now(timezone.utc) - b.created_at).total_seconds() < 14400:
                logger.info(f"ğŸ”„ æ•‘æ¸ˆå¯¾è±¡ãƒãƒƒãƒã‚’ç™ºè¦‹: {b.id}")
                batch_id = b.id; break

        if not batch_id:
            # 1. ãƒªãƒ³ã‚¯åé›†
            logger.info("ã€ã‚¹ãƒ†ãƒƒãƒ—1ã€‘å…¨å›½ãƒªãƒ³ã‚¯åé›†é–‹å§‹")
            prefecture_results = search_all_prefectures_direct()
            all_tasks = [{"pref": p, **r} for p, rs in prefecture_results.items() for r in rs]
            logger.info(f"âœ… {len(all_tasks)}ä»¶ã®ãƒªãƒ³ã‚¯ã‚’å–å¾—")
            
            # 2. ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡º
            logger.info("ã€ã‚¹ãƒ†ãƒƒãƒ—2ã€‘é‡è¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºé–‹å§‹")
            batch_requests = []
            for i, task in enumerate(all_tasks, 1):
                if i % 20 == 0: logger.info(f"é€²æ—: {i}/{len(all_tasks)}")
                content_data = extractor.extract(task['url'])
                if not content_data: continue
                
                cid = f"req_{i}"
                # æ•‘æ¸ˆç”¨ã« content ã‚‚ä¿æŒã™ã‚‹ã‚ˆã†è¿½åŠ 
                url_map[cid] = {**task, 'content': content_data['content']}
                batch_requests.append(analyzer.make_batch_request(cid, task['title'], content_data['content'], task['url']))
            
            # 3. Batché€ä¿¡
            logger.info(f"ã€ã‚¹ãƒ†ãƒƒãƒ—3ã€‘Anthropicé€ä¿¡ ({len(batch_requests)}ä»¶)")
            batch = analyzer.client.beta.messages.batches.create(requests=batch_requests)
            batch_id = batch.id
        else:
            logger.info("â­ï¸ åé›†ã‚¹ãƒ†ãƒƒãƒ—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã€è§£æçµæœã®å‡¦ç†ã¸é€²ã¿ã¾ã™")

        # 4. å®Œäº†å¾…æ©Ÿ ï¼‹ 5æ™‚é–“ã‚¿ã‚¤ãƒãƒ¼
        logger.info("ã€ã‚¹ãƒ†ãƒƒãƒ—4ã€‘AIè§£æå¾…ã¡...")
        use_fallback = False
        while True:
            try:
                b_status = analyzer.client.beta.messages.batches.retrieve(batch_id)
                if b_status.processing_status == "ended": break
                
                # ğŸ†• 5æ™‚é–“(18,000ç§’)ã‚’è¶…ãˆãŸã‹åˆ¤å®š
                if (time.time() - start_time) > 18000:
                    logger.warning("âš ï¸ 5æ™‚é–“ã‚’çµŒéã—ã¦ã‚‚BatchãŒå®Œäº†ã—ã¾ã›ã‚“ã€‚é€šå¸¸APIã«ã‚ˆã‚‹å³æ™‚è§£æã«åˆ‡ã‚Šæ›¿ãˆã¾ã™ã€‚")
                    use_fallback = True
                    break
                
                logger.info(f"â³ {b_status.processing_status}: {b_status.request_counts.succeeded}ä»¶å®Œäº†")
                time.sleep(60)
            except Exception as e:
                logger.warning(f"âš ï¸ 5åˆ†å¾…æ©Ÿ... ({e})"); time.sleep(300)
        
        # 5. çµæœè§£æ
        logger.info("ã€ã‚¹ãƒ†ãƒƒãƒ—5ã€‘çµæœè§£æä¸­...")
        final_projects, stats = [], {"A": 0, "B": 0, "C": 0}
        seen_titles = set()

        if use_fallback:
            # ğŸ†• æ•‘æ¸ˆãƒ¢ãƒ¼ãƒ‰ï¼šã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦é€šå¸¸APIã‚’å©ã
            if not url_map:
                logger.error("âŒ æ•‘æ¸ˆãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆã¾ã—ãŸãŒã€è§£æç”¨ãƒ‡ãƒ¼ã‚¿ãŒãƒ¡ãƒ¢ãƒªã«ã‚ã‚Šã¾ã›ã‚“ã€‚å†å®Ÿè¡ŒãŒå¿…è¦ã§ã™ã€‚")
            else:
                for cid, task in url_map.items():
                    analysis = analyzer.analyze_single(task['title'], task['content'], task['url'])
                    if analysis:
                        label = analysis.get('label', 'C')
                        t = analysis.get('title', 'ç„¡é¡Œ')
                        if label in ["A", "B"] and t not in seen_titles:
                            logger.info(f"âœ¨ é€šå¸¸APIæ•‘æ¸ˆåˆ¤å®š({label}): {t}")
                            analysis.update({'prefecture': task['pref']})
                            final_projects.append(analysis)
                            seen_titles.add(t)
                    time.sleep(1) # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
        else:
            # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ï¼šBatchã®çµæœã‚’å‡¦ç†
            for res in analyzer.client.beta.messages.batches.results(batch_id):
                if res.result.type == "succeeded":
                    try:
                        analysis = json.loads(re.search(r'\{.*\}', res.result.message.content[0].text, re.DOTALL).group(0))
                        label = analysis.get('label', 'C')
                        stats[label] = stats.get(label, 0) + 1
                        t = analysis.get('title', 'ç„¡é¡Œ')
                        
                        if t in seen_titles: continue
                        if label in ["A", "B"]:
                            # æœ€çµ‚æ¤œé–²ï¼ˆå¹´åº¦ãƒ»æœŸé™ï¼‰
                            if re.search(r"ä»¤å’Œ[5-7]|R[5-7]|202[3-5]", t) and "ä»¤å’Œ8" not in t: continue
                            dp = analysis.get('deadline_prop', 'ä¸æ˜')
                            if dp != "ä¸æ˜":
                                m = re.search(r'(\d{4})[-/å¹´](\d{1,2})[-/æœˆ](\d{1,2})', dp)
                                if m and datetime(*map(int, m.groups())).date() < today: continue

                            logger.info(f"âœ… åˆæ ¼åˆ¤å®š({label}): {t}")
                            seen_titles.add(t)
                            
                            # URL/éƒ½é“åºœçœŒã®å¾©å…ƒ
                            if res.custom_id in url_map:
                                analysis.update({'prefecture': url_map[res.custom_id]['pref'], 'source_url': url_map[res.custom_id]['url']})
                            
                            final_projects.append(analysis)
                    except: continue

        # 6. ã‚·ãƒ¼ãƒˆæ›¸ãè¾¼ã¿
        if final_projects:
            sheet_name = datetime.now(jst).strftime("æ˜ åƒæ¡ˆä»¶_%Yå¹´%mæœˆ_v16")
            sheets_manager.append_projects(sheets_manager.prepare_v12_sheet(sheet_name), final_projects)
            logger.info(f"âœ¨ å®Œäº†ï¼ {len(final_projects)}ä»¶ã‚’è¿½åŠ ã—ã¾ã—ãŸ")
        else: logger.warning("âš ï¸ æ–°ç€æ¡ˆä»¶ãªã—")
        
    except Exception as e: logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    main()
