import logging
import os
import json
import time
import re
from datetime import datetime, timezone, timedelta
from analyzer.ai_analyzer import AIAnalyzer
from database.sheets_manager import SheetsManager

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def main():
    logger.info("=" * 60)
    logger.info("æ˜ åƒæ¡ˆä»¶ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° v1.26 [ã‚¾ãƒ³ãƒ“æ¡ˆä»¶æ’é™¤ãƒ»æœ€çµ‚å®Œæˆç‰ˆ]")
    logger.info("=" * 60)
    
    try:
        from scrapers.direct_scraper import search_all_prefectures_direct
        from scrapers.content_extractor import ContentExtractor
        
        analyzer = AIAnalyzer()
        sheets_manager = SheetsManager(os.environ["SPREADSHEET_ID"], json.loads(os.environ["GCP_SERVICE_ACCOUNT"]))
        extractor = ContentExtractor()
        jst = timezone(timedelta(hours=9))
        today = datetime.now(jst).date()

        logger.info("ã€ã‚¹ãƒ†ãƒƒãƒ—1ã€‘å…¨å›½ãƒªãƒ³ã‚¯åé›†é–‹å§‹...")
        prefecture_results = search_all_prefectures_direct()
        all_tasks = [{"pref": p, **r} for p, rs in prefecture_results.items() for r in rs]
        
        final_projects = []
        seen_titles = set()
        
        logger.info("ã€ã‚¹ãƒ†ãƒƒãƒ—2ã€‘æ¡ˆä»¶é¸åˆ¥ï¼ˆå¿œå‹ŸæœŸé™ã‚’å³æ ¼ã«ãƒã‚§ãƒƒã‚¯ï¼‰...")
        for i, task in enumerate(all_tasks, 1):
            url = task['url']
            title_raw = task['title']

            if re.search(r"youtube\.com|youtu\.be|facebook\.com|instagram\.com|x\.com|twitter\.com", url): continue

            # --- ğŸ›¡ï¸ é–€ç•ª1ï¼šã‚¿ã‚¤ãƒˆãƒ«ã®ã€Œå†·å¾¹ãªæ’é™¤ã€ (æ±ºå®šãƒ»çµ‚äº†ã®æ°—é…ã‚’å¯ŸçŸ¥) ---
            if re.search(r"æ±ºå®š|å…¬è¡¨|é¸å®š|è½æœ­|çµæœ|å¯©æŸ»|å ±å‘Š|å®Ÿç¸¾|æˆåŠŸ|é”æˆ|å…¬é–‹|å®Œäº†|åˆ¶ä½œã—ã¾ã—ãŸ|æ”¾æ˜ ä¸­|é…ä¿¡ä¸­|çµ‚äº†", title_raw):
                continue
            if re.search(r"æ¡ç”¨|è·å“¡|è–¬å‰¤å¸«|è­¦å¯Ÿ|æ•™å“¡|çœ‹è­·|åŒ»å¸«|è©¦é¨“|ç›¸è«‡|å€‹äºº|è¬›ç¿’", title_raw):
                continue

            # --- ğŸ›¡ï¸ é–€ç•ª2ï¼šã€æ•‘æ¸ˆã€‘æœ¬ç‰©ã‚’é€ƒã•ãªã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ ---
            if not re.search(r"å‹Ÿé›†|å§”è¨—|å…¥æœ­|ãƒ—ãƒ­ãƒãƒ¼ã‚¶ãƒ«|ã‚³ãƒ³ãƒš|å…¬å‹Ÿ|ä¼ç”»ææ¡ˆ|åˆ¶ä½œ|ä½œæˆ|æ’®å½±|æ¥­å‹™|å‹•ç”»|PR|ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³", title_raw):
                continue

            if i % 20 == 0: logger.info(f"é€²æ—: {i}/{len(all_tasks)} ä»¶å®Œäº†")
            
            content_data = extractor.extract(url)
            if not content_data: continue
            raw_text = content_data['content']

            # --- ğŸ›¡ï¸ é–€ç•ª3ï¼šæœ¬æ–‡ã®å¹´åº¦æ¤œé–² (2026å¹´ã¾ãŸã¯ä»¤å’Œ8å¹´ã®æ°—é…ã‚’ç¢ºèª) ---
            # 1æœˆç¾åœ¨ãªã®ã§ã€ä»¤å’Œ7å¹´åº¦ã®äºˆç®—ã§ã€Œä»¤å’Œ8å¹´ã®ä»•äº‹ã€ã‚’å‹Ÿé›†ã—ã¦ã„ã‚‹ã‚±ãƒ¼ã‚¹ã‚‚å¤šã„ãŸã‚ã€R7ã¨R8ã®ä¸¡æ–¹ã‚’è¦–é‡ã«å…¥ã‚Œã¾ã™
            if not re.search(r"ä»¤å’Œ[78]|R[78]|202[56]", raw_text):
                continue

            analysis = analyzer.analyze_single(title_raw, raw_text, url)
            if not analysis: continue
            
            # --- ğŸ›¡ï¸ é–€ç•ª4ï¼šAIå›ç­”ã®ã€ŒæœŸé™ã€ã‚’å¾¹åº•æ¤œé–² ---
            if analysis.get('label') not in ["A", "B"]: continue
            title = analysis.get('title', 'ç„¡é¡Œ')
            if title in seen_titles: continue

            evidence = analysis.get('evidence','')
            memo = analysis.get('memo','')
            full_ans = f"{title} {evidence} {memo}"

            # â‘  AIãŒã€ŒæœŸé™åˆ‡ã‚Œã€ã‚„ã€Œå‹Ÿé›†çµ‚äº†ã€ã‚’èªã‚ã¦ã„ã‚‹å ´åˆ
            if re.search(r"çµ‚äº†ã—ã¦ã„ã¾ã™|æœŸé™ãŒéãã¦|éå»ã®æ¡ˆä»¶", full_ans):
                continue

            # â‘¡ å¿œå‹Ÿç· åˆ‡ã®å³æ ¼ãƒã‚§ãƒƒã‚¯ (ã‚¾ãƒ³ãƒ“æ¡ˆä»¶å¯¾ç­–)
            # å‚åŠ ç”³è¾¼ã¨ææ¡ˆæ›¸ã®ã©ã¡ã‚‰ã‹ä¸€æ–¹ãŒä»Šæ—¥ä»¥é™ã§ã‚ã‚‹ã“ã¨
            deadline_date = None
            d1 = analysis.get('deadline_apply', 'ä¸æ˜')
            d2 = analysis.get('deadline_prop', 'ä¸æ˜')
            
            dates_to_check = []
            for d_str in [d1, d2]:
                if d_str and d_str != "ä¸æ˜":
                    m = re.search(r'(\d{4})[-/å¹´](\d{1,2})[-/æœˆ](\d{1,2})', d_str)
                    if m:
                        dates_to_check.append(datetime(int(m.group(1)), int(m.group(2)), int(m.group(3))).date())
            
            if dates_to_check:
                # è¦‹ã¤ã‹ã£ãŸã™ã¹ã¦ã®æ—¥ä»˜ãŒä»Šæ—¥ã‚ˆã‚Šå‰ãªã‚‰ã€ŒæœŸé™åˆ‡ã‚Œã€ã¨ã—ã¦æ¨ã¦ã‚‹
                if all(d < today for d in dates_to_check):
                    logger.info(f"âŒ› æœŸé™åˆ‡ã‚Œ(ã‚¾ãƒ³ãƒ“)é™¤å¤–: {title}")
                    continue
            else:
                # æ—¥ä»˜ãŒä¸€åˆ‡ä¸æ˜ã§ã€ã‹ã¤æœ¬æ–‡ã«ã€Œä»¤å’Œ8å¹´ã€ã®å‹Ÿé›†ã®æ°—é…ãŒãªã„ã‚‚ã®ã‚‚æ€ªã—ã„ã®ã§æ¨ã¦ã‚‹
                if "ä»¤å’Œ8" not in full_ans and "2026" not in full_ans:
                    continue

            # --- âœ¨ æœ€çµ‚åˆæ ¼ï¼š2026å¹´ã«å‘ã‘ãŸã€Œä»Šã€å¿œå‹Ÿã§ãã‚‹ã€çœŸã®æ¡ˆä»¶ ---
            analysis.update({'prefecture': task['pref']})
            final_projects.append(analysis)
            seen_titles.add(title)
            logger.info(f"ğŸ¯ æœ‰åŠ¹æ¡ˆä»¶ã‚’æ•æ‰: {title}")
            time.sleep(0.1)

        if final_projects:
            sheet_name = datetime.now(jst).strftime("æ˜ åƒæ¡ˆä»¶_%Yå¹´%mæœˆ_v16")
            sheets_manager.append_projects(sheets_manager.prepare_v12_sheet(sheet_name), final_projects)
            logger.info(f"âœ¨ å®Œäº†ï¼ çœŸã®æœ‰åŠ¹æ¡ˆä»¶ {len(final_projects)}ä»¶ã‚’è¿½åŠ ã—ã¾ã—ãŸ")
        else:
            logger.warning("âš ï¸ å‹Ÿé›†ä¸­ã®æœ‰åŠ¹æ¡ˆä»¶ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    main()
