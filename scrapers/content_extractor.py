import requests
import logging
import io
import re
import pdfplumber
from bs4 import BeautifulSoup
from typing import Dict, Optional
from urllib.parse import urljoin
import urllib3

# SSLã‚¨ãƒ©ãƒ¼å¯¾ç­–
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
logger = logging.getLogger(__name__)

class ContentExtractor:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self.verify = False 

    def extract(self, url: str) -> Optional[Dict]:
        try:
            response = self.session.get(url, timeout=30, verify=self.verify)
            response.encoding = response.apparent_encoding
            soup = BeautifulSoup(response.content, 'html.parser')
            
            for tag in soup(['script', 'style', 'nav', 'header', 'footer']):
                tag.decompose()
            
            # Webãƒšãƒ¼ã‚¸æœ¬æ–‡
            main_text = f"ã€Webæœ¬æ–‡ã€‘\n{soup.get_text(separator=' ', strip=True)[:3000]}\n"
            
            # PDFãƒªãƒ³ã‚¯ã®æŠ½å‡ºã¨é¸åˆ¥
            combined_pdf_text = ""
            for link in soup.find_all('a', href=True):
                href = link['href']
                link_text = link.get_text()
                
                if href.lower().endswith('.pdf'):
                    # ðŸ†• ãƒŽã‚¤ã‚ºPDFï¼ˆçµæžœã€å›žç­”ã€æ§˜å¼ãªã©ï¼‰ã¯èª­ã¿é£›ã°ã™
                    if any(x in link_text for x in ['è³ªå•', 'å›žç­”', 'çµæžœ', 'è½æœ­', 'æ§˜å¼', 'è¨˜å…¥ä¾‹', 'åç°¿']):
                        continue
                        
                    pdf_url = urljoin(url, href)
                    combined_pdf_text += self._extract_future_pages(pdf_url)
                    
                    if len(main_text + combined_pdf_text) > 12000: break

            return {'url': url, 'content': main_text + combined_pdf_text}
        except Exception as e:
            logger.error(f"æŠ½å‡ºå¤±æ•—: {url} - {e}")
            return None

    def _extract_future_pages(self, pdf_url):
        """PDFå…¨ãƒšãƒ¼ã‚¸ã‚’èµ°æŸ»ã—ã€2026å¹´(R8)ä»¥é™ã®è¨˜è¿°ã‚„ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒã‚ã‚‹ãƒšãƒ¼ã‚¸ã‚’æŠœç²‹"""
        try:
            res = self.session.get(pdf_url, timeout=20, verify=self.verify)
            extracted = f"\n--- PDF: {pdf_url.split('/')[-1]} ---\n"
            with pdfplumber.open(io.BytesIO(res.content)) as pdf:
                for page in pdf.pages:
                    text = page.extract_text() or ""
                    # ðŸ†• 2026å¹´ä»¥é™ã€ä»¤å’Œ8å¹´ä»¥é™ã€ã¾ãŸã¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å˜èªžã‚’æ¤œç´¢
                    future_yr = re.search(r"(202[6-9]|20[3-9][0-9]|ä»¤å’Œ[8-9]|ä»¤å’Œ[1-2][0-9]|R[8-9]|R[1-2][0-9])", text)
                    is_sch = any(k in text for k in ["ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«", "æœŸé–“", "æœŸé™", "ç· åˆ‡", "æå‡º", "å®Ÿæ–½"])
                    if future_yr or is_sch:
                        extracted += text + "\n"
                        if len(extracted) > 4000: break
            return extracted
        except: return ""
