"""HTML/PDFã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºï¼ˆPyMuPDFæ·±å±¤è§£æã‚¨ãƒ³ã‚¸ãƒ³é€£çµç‰ˆï¼‰"""

import requests
import logging
from bs4 import BeautifulSoup
from typing import Dict, Optional
from urllib.parse import urljoin
import urllib3
from scrapers.pdf_handler import PDFHandler  # æ–°ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

# SSLã‚¨ãƒ©ãƒ¼å¯¾ç­–
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

logger = logging.getLogger(__name__)

class ContentExtractor:
    """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self.verify = False 
        # Step 1 ã§ä½œæˆã—ãŸPDFæ·±å±¤è§£æã‚¨ãƒ³ã‚¸ãƒ³ã‚’åˆæœŸåŒ–ï¼ˆæœ€å¤§30ãƒšãƒ¼ã‚¸ï¼‰
        self.pdf_handler = PDFHandler(max_pages=30)

    def extract(self, url: str) -> Optional[Dict]:
        """URLã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºï¼ˆPDFãªã‚‰æ·±å±¤è§£æã€HTMLãªã‚‰ä¸­ã®PDFã‚‚æ·±æ˜ã‚Šï¼‰"""
        logger.info(f"ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºé–‹å§‹: {url}")
        
        # 1. URLè‡ªä½“ãŒPDFã‚’æŒ‡ã—ã¦ã„ã‚‹å ´åˆ
        if url.lower().endswith('.pdf'):
            return self._extract_pdf_deep(url)
        
        # 2. HTMLãƒšãƒ¼ã‚¸ã®å ´åˆï¼ˆãƒšãƒ¼ã‚¸å†…ã®é‡è¦PDFã‚‚æ·±æ˜ã‚Šã™ã‚‹ï¼‰
        return self._extract_html_with_deep_peek(url)

    def _extract_pdf_deep(self, url: str) -> Optional[Dict]:
        """æ–°ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½¿ç”¨ã—ã¦PDFã‚’æœ€å¤§30ãƒšãƒ¼ã‚¸ã¾ã§è§£æã™ã‚‹"""
        text = self.pdf_handler.extract_text_from_url(url)
        if not text:
            return None
            
        return {
            'url': url,
            'title': url.split('/')[-1],
            'content': text, # ã“ã“ã§ã¯ã‚«ãƒƒãƒˆã›ãšã€AIã«æ¸¡ã™ç›´å‰ã§åˆ¶å¾¡ã—ã¾ã™
            'file_type': 'pdf'
        }

    def _extract_html_with_deep_peek(self, url: str) -> Optional[Dict]:
        """HTMLã‚’è§£æã—ã€é–¢é€£PDFãŒã‚ã‚Œã°æ–°ã‚¨ãƒ³ã‚¸ãƒ³ã§å…¨ãƒšãƒ¼ã‚¸è§£æã—ã¦åˆæµã•ã›ã‚‹"""
        try:
            response = self.session.get(url, timeout=30, verify=self.verify)
            response.raise_for_status()
            # æ–‡å­—åŒ–ã‘å¯¾ç­–
            response.encoding = response.apparent_encoding
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã®å–å¾—
            title = soup.title.string.strip() if soup.title else url
            
            # ä¸è¦ãªã‚¿ã‚°ã‚’é™¤å»
            for tag in soup(['script', 'style', 'nav', 'header', 'footer']):
                tag.decompose()
            main_text = soup.get_text(separator='\n', strip=True)
            
            # --- æ·±æ˜ã‚Šæ©Ÿèƒ½ï¼šé‡è¦ãªPDFãƒªãƒ³ã‚¯ã‚’1ã¤ã ã‘ã€Œå…¨ãƒšãƒ¼ã‚¸ã€è§£æã™ã‚‹ ---
            extra_pdf_text = ""
            for link in soup.find_all('a', href=True):
                href = link['href']
                link_text = link.get_text()
                
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«åˆè‡´ã™ã‚‹PDFã‚’æ¢ã™
                if href.lower().endswith('.pdf') and any(k in link_text for k in ['è¦é ˜', 'å‹Ÿé›†', 'æ¦‚è¦', 'ä»•æ§˜', 'æŒ‡é‡']):
                    pdf_url = urljoin(url, href)
                    logger.info(f"ğŸ” é‡è¦è³‡æ–™PDFã‚’æ·±å±¤è§£æã—ã¾ã™: {link_text}")
                    
                    # ä»¥å‰ã®5ãƒšãƒ¼ã‚¸åˆ¶é™ã‚’æ’¤å»ƒã—ãŸæ–°ã‚¨ãƒ³ã‚¸ãƒ³ã§è§£æ
                    pdf_data = self._extract_pdf_deep(pdf_url)
                    if pdf_data and pdf_data['content']:
                        extra_pdf_text = f"\n\n--- ä»˜å±è³‡æ–™PDF({link_text})ã®å…¨å®¹ ---\n{pdf_data['content']}"
                        break # æœ€ã‚‚é‡è¦ãª1ã¤ã‚’æ·±æ˜ã‚Šã—ãŸã‚‰çµ‚äº†

            return {
                'url': url,
                'title': title,
                'content': main_text + extra_pdf_text,
                'file_type': 'html'
            }
            
        except Exception as e:
            logger.error(f"HTMLæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {url} - {e}")
            return None
